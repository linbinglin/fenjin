import streamlit as st
import openai
import re

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="å…¨èƒ½æ–‡æ¡ˆÂ·ç”µå½±æ„Ÿåˆ†é•œç³»ç»Ÿ (ä¸¥æ ¼ç‰ˆ)", layout="wide")

# è‡ªå®šä¹‰ CSS è®©ç•Œé¢æ›´ä¸“ä¸š
st.markdown("""
    <style>
    .stMetric { background-color: #f0f2f6; padding: 10px; border-radius: 10px; }
    .error-text { color: red; font-weight: bold; }
    </style>
""", unsafe_allow_html=True)

st.title("ğŸ¬ å…¨èƒ½æ–‡æ¡ˆÂ·ç”µå½±æ„Ÿåˆ†é•œç³»ç»Ÿ (V1.2)")

# --- ä¾§è¾¹æ é…ç½® (è¿™é‡Œæ˜¯ä¿®æ­£é‡ç‚¹) ---
with st.sidebar:
    st.header("âš™ï¸ å¯¼æ¼”å¼•æ“é…ç½®")
    api_key = st.text_input("1. API Key", type="password", placeholder="sk-...")
    base_url = st.text_input("2. æ¥å£åœ°å€", value="https://blog.tuiwen.xyz/v1")
    
    # ä¿®æ­£ï¼šæ”¹ä¸ºæ‰‹åŠ¨è¾“å…¥ï¼Œä»¥ä¾¿é€‚é…å„ç§ä¸­è½¬æ¥å£çš„æ¨¡å‹ä»£å·
    model_id = st.text_input("3. Model ID (è¯·æ‰‹åŠ¨è¾“å…¥æ¨¡å‹åç§°)", value="grok-1", placeholder="ä¾‹å¦‚: gpt-4o, deepseek-chat, grok-1")
    
    st.divider()
    st.markdown("""
    **V1.2 è§†è§‰åˆ‡å‰²é€»è¾‘ï¼š**
    - **å¼ºåˆ¶æµå¼åŒ–**ï¼šåˆ é™¤åŸæ–‡æ‰€æœ‰æ¢è¡Œã€‚
    - **ç¡¬æ€§35å­—**ï¼šå•é•œå¤´æ–‡æœ¬ç¦è¶…35å­—ã€‚
    - **é€»è¾‘æ–­ç‚¹**ï¼šäººç§°/åŠ¨ä½œ/åœºæ™¯åˆ‡æ¢å¿…æ–­ã€‚
    - **é›¶å¢åˆ **ï¼šåŸæ–‡å†…å®¹æ— æŸè¿˜åŸã€‚
    """)

# --- æ ¸å¿ƒé€»è¾‘å¤„ç† ---
def generate_storyboard(text, api_key, base_url, model):
    # æ­¥éª¤ 1: æ–‡æœ¬æç«¯é¢„å¤„ç† - å½»åº•åˆ æ‰æ‰€æœ‰æ¢è¡Œå’Œå¤šä½™ç©ºæ ¼ï¼Œç¡®ä¿ AI æ— æ³•å‚è€ƒåŸæ®µè½
    clean_text = "".join(text.split())
    
    # é€‚é… Base URL æ ¼å¼
    client = openai.OpenAI(api_key=api_key, base_url=base_url)
    
    # ä¸¥å¯†çš„ Prompt é€»è¾‘
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªåˆ†é•œè¯­è¨€ä¸“å®¶ã€‚ä½ å”¯ä¸€çš„è¾“å‡ºèŒè´£æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆå¸¦ç¼–å·çš„çŸ­å¥ã€‚\n"
        "ä¸¥æ ¼è§„åˆ™ï¼š\n"
        "1. å¿…é¡»åŒ…å«åŸæ–‡æ‰€æœ‰å­—è¯ï¼Œä¸¥ç¦å¢åŠ ä»»ä½•è§£é‡Šã€ä¸¥ç¦åˆ å‡ä»»ä½•å­—è¯ã€ä¸¥ç¦æ”¹å†™ã€‚\n"
        "2. æ¯ä¸€ä¸ªåˆ†é•œçš„æ–‡æ¡ˆä¸èƒ½è¶…è¿‡35ä¸ªå­—ã€‚\n"
        "3. åˆ‡åˆ†æ—¶æœºï¼šåœºæ™¯åˆ‡æ¢ã€è§’è‰²åˆ‡æ¢ã€æ–°åŠ¨ä½œå‘ç”Ÿã€æˆ–è¾¾åˆ°å­—æ•°ä¸Šé™ã€‚\n"
        "4. **ç¦æ­¢è¾“å‡ºä»»ä½•å¼€å¤´è¯­æˆ–ç»“æŸè¯­**ï¼ˆå¦‚'å¥½çš„'ã€'å¦‚ä¸‹'ï¼‰ã€‚\n"
        "5. æ ¼å¼ï¼š1.å†…å®¹\\n2.å†…å®¹"
    )
    
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·å¯¹ä»¥ä¸‹æµå¼æ–‡æœ¬è¿›è¡Œæ— æŸåˆ†é•œåˆ‡åˆ†ï¼š\n{clean_text}"}
            ],
            temperature=0.1 # æç«¯ä½é‡‡æ ·ï¼Œç¡®ä¿é€»è¾‘ä¸¥è°¨ï¼Œä¸äº§ç”Ÿå¹»è§‰
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

# --- ä¸»ç•Œé¢ ---
uploaded_file = st.file_uploader("ğŸ“‚ ä¸Šä¼  TXT å‰§æœ¬", type=['txt'])

if uploaded_file:
    # å¼ºåˆ¶ UTF-8 è¯»å–
    raw_content = uploaded_file.read().decode("utf-8")
    # è¿‡æ»¤æ‰åŸæ–‡ä¸­çš„ç©ºè¡Œè®¡ç®—çº¯å­—æ•°
    pure_raw_text = "".join(raw_content.split())
    raw_len = len(pure_raw_text)

    col_input, col_output = st.columns(2)
    
    with col_input:
        st.subheader("ğŸ“„ åŸå§‹æ–‡æœ¬è®°å½•")
        st.text_area("Input", raw_content, height=400)
        st.metric("åŸæ–‡æ€»å­—æ•° (ä¸å«ç©ºæ ¼)", raw_len)

    if st.button("ğŸš€ å¯åŠ¨è§†è§‰é€»è¾‘ç¨½æ ¸"):
        if not api_key or not model_id:
            st.warning("âš ï¸ è¯·å…ˆå®Œå–„ä¾§è¾¹æ çš„ API Key å’Œ Model ID")
        else:
            with st.spinner(f"æ­£åœ¨è°ƒç”¨ {model_id} è¿›è¡Œæ·±åº¦é€»è¾‘åˆ‡åˆ†..."):
                result = generate_storyboard(raw_content, api_key, base_url, model_id)
                
                with col_output:
                    st.subheader("ğŸ“½ï¸ è§†è§‰åˆ†é•œç¼–è¾‘å™¨")
                    st.text_area("Output", result, height=400)
                    
                    # ä¸¥è°¨æ€§æ ¡éªŒï¼šæå–è¾“å‡ºä¸­çš„æ‰€æœ‰æ±‰å­—/ç¬¦å·ï¼Œä¸åŸæ–‡å¯¹æ¯”
                    # ç§»é™¤æ•°å­—ç¼–å·å’Œæ¢è¡Œè¿›è¡Œç»Ÿè®¡
                    processed_text = re.sub(r'\d+\.\s*|\n', '', result)
                    processed_len = len(processed_text)
                    
                    st.metric("å¤„ç†åæ€»å­—æ•°", processed_len)
                    
                    diff = raw_len - processed_len
                    if diff == 0:
                        st.success("âœ… é€»è¾‘å®Œç¾ï¼šå­—æ•°å®Œå…¨åŒ¹é…ï¼Œæ— æŸè¿˜åŸã€‚")
                    else:
                        st.error(f"âŒ é€»è¾‘å¼‚å¸¸ï¼šåå·®å€¼ {diff} å­—ã€‚AIå¯èƒ½äº§ç”Ÿäº†å¹»è§‰æˆ–å·æ‡’ã€‚")
                        st.write(f"å»ºè®®ï¼šæ£€æŸ¥ Model ID æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•æ›´é«˜å‚æ•°æ¨¡å‹ã€‚")

    # æ‰©å±•åˆ†ææ¨¡å—ï¼ˆæ¨¡ä»¿æ¡ˆä¾‹å›¾ï¼‰
    if 'result' in locals() and "Error" not in result:
        st.divider()
        st.subheader("ğŸ“Š å®æ—¶è§†è§‰èŠ‚å¥åˆ†æ")
        lines = result.split('\n')
        groups_count = len([l for l in lines if l.strip()])
        
        c1, c2, c3 = st.columns(3)
        c1.metric("ç”Ÿæˆåˆ†é•œæ€»æ•°", f"{groups_count} ç»„")
        c2.metric("å¹³å‡æ¯é•œæ—¶é•¿", f"{round(raw_len/groups_count, 1)} å­—", help="å»ºè®®åœ¨35å­—ä»¥å†…")
        c3.metric("èŠ‚å¥è¯„ä¼°", "ä¼˜" if raw_len/groups_count < 30 else "ç•¥æ˜¾ç–²åŠ³")
