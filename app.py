import streamlit as st
from openai import OpenAI
import io
import re
import pandas as pd

st.set_page_config(page_title="AIåˆ†é•œå¯¼æ¼”Pro", layout="wide")

# --- åˆå§‹åŒ–çŠ¶æ€ ---
if 'processed_text' not in st.session_state: st.session_state.processed_text = ""
if 'original_stream' not in st.session_state: st.session_state.original_stream = ""
if 'desc_results' not in st.session_state: st.session_state.desc_results = []
if 'current_batch' not in st.session_state: st.session_state.current_batch = 0

# --- é…ç½®åŒº ---
st.sidebar.title("âš™ï¸ å¯¼æ¼”å®¤é…ç½®")
api_key = st.sidebar.text_input("API Key", type="password")
base_url = st.sidebar.text_input("æ¥å£åœ°å€", value="https://blog.tuiwen.xyz/v1")
model_id = st.sidebar.text_input("æ¨¡å‹é€‰æ‹©", value="gpt-4o") # å»ºè®®ä½¿ç”¨å¼ºåŠ›æ¨¡å‹

st.title("ğŸ¬ ç”µå½±è§£è¯´å…¨æµç¨‹åˆ†é•œå·¥å…·")
st.markdown("---")

# ================= ç¬¬ä¸€é˜¶æ®µï¼šç‰©ç†ç²‰ç¢ä¸æ— æŸé‡æ„ =================
st.header("Step 1: æ–‡æœ¬å»æ ¼å¼åŒ–ä¸èŠ‚å¥é‡ç»„")

uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡ˆ (TXT)", type=['txt'])

if uploaded_file:
    # ã€æ ¸å¿ƒæ“ä½œã€‘å½»åº•ç‰©ç†åˆ é™¤åŸæ®µè½ç»“æ„
    raw_content = io.StringIO(uploaded_file.getvalue().decode("utf-8")).read()
    # è¿‡æ»¤æ‰æ‰€æœ‰æ¢è¡Œã€ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦
    clean_stream = re.sub(r'[\s\n\r\t]+', '', raw_content).strip()
    st.session_state.original_stream = clean_stream
    
    st.info(f"å·²ç‰©ç†ç²‰ç¢åŸæ®µè½ã€‚å½“å‰å¾…å¤„ç†å­—ç¬¦æµæ€»é•¿ï¼š{len(clean_stream)} å­—ã€‚")

    if st.button("ğŸš€ å¼ºåˆ¶æ™ºèƒ½åˆ†é•œï¼ˆæ‰“ç ´åŸæ–‡ç»“æ„ï¼‰"):
        if not api_key: st.error("è¯·é…ç½®API Key")
        else:
            try:
                client = OpenAI(api_key=api_key, base_url=base_url)
                
                # é’ˆå¯¹â€œå¤ªç¢â€å’Œâ€œå·æ‡’â€å®šåˆ¶çš„æç«¯Prompt
                director_instruction = f"""ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ç”µå½±å‰ªè¾‘å¯¼æ¼”ã€‚ç°åœ¨æˆ‘ç»™ä½ ä¸€æ®µå®Œå…¨æ²¡æœ‰æ®µè½çš„å­—ç¬¦æµï¼Œè¯·ä½ è¿›è¡Œåˆ†é•œåˆ‡å‰²ã€‚

### ä½ çš„æ ¸å¿ƒåˆ†é•œæŠ€å·§ï¼š
1. **è¯­ä¹‰èšæ‹¢ï¼ˆé˜²æ­¢å¤ªç¢ï¼‰**ï¼šä¸€ä¸ªåˆ†é•œä»£è¡¨5ç§’è§†é¢‘ã€‚å¦‚æœä¸€å¥è¯å¾ˆçŸ­ï¼ˆå¦‚â€œä»–èµ°äº†è¿‡æ¥â€ï¼‰ï¼Œä¸¥ç¦å•ç‹¬åˆ†é•œï¼å¿…é¡»æŠŠå®ƒå’Œåç»­çš„åŠ¨ä½œï¼ˆå¦‚â€œååœ¨äº†æ²™å‘ä¸Šï¼Œç‚¹ç‡ƒäº†ä¸€æ ¹çƒŸâ€ï¼‰åˆå¹¶ï¼Œåªè¦æ€»é•¿ä¸è¶…è¿‡40å­—ï¼Œå°½é‡è®©åˆ†é•œæ–‡æ¡ˆé¥±æ»¡ï¼Œç¡®ä¿ç”»é¢æœ‰åŠ¨ä½œè·¨åº¦ã€‚
2. **ç¡¬æ€§è¾¹ç•Œ**ï¼šæ¯ä¸ªåˆ†é•œæ–‡æ¡ˆä¸¥æ ¼é™åˆ¶åœ¨ 30-40 å­—ç¬¦ã€‚ç»å¯¹ç¦æ­¢è¶…è¿‡40ä¸ªå­—ç¬¦ï¼Œå¦åˆ™è§†é¢‘æ—¶é•¿ä¸å¤Ÿã€‚
3. **å¼ºåˆ¶åˆ‡åˆ†ç‚¹**ï¼šåªæœ‰åœ¨ã€è§’è‰²å˜æ¢å¯¹è¯ã€‘æˆ–ã€åœºæ™¯å½»åº•æ”¹å˜ã€‘æ—¶ï¼Œå³ä½¿å­—æ•°å¾ˆå°‘ä¹Ÿå¿…é¡»åˆ‡åˆ†ã€‚
4. **æ— æŸè¦æ±‚**ï¼šä¸¥ç¦ä¿®æ”¹ã€æ·»åŠ æˆ–åˆ é™¤ä»»ä½•å­—ç¬¦ã€‚ä½ åªæ˜¯åœ¨é•¿å¥ä¸­æ’å…¥æ¢è¡Œç¬¦ã€‚

### å¾…å¤„ç†å­—ç¬¦æµï¼š
{clean_stream}"""

                with st.spinner("AIæ­£åœ¨é‡æ–°è§£æ„å‰§æƒ…èŠ‚å¥..."):
                    response = client.chat.completions.create(
                        model=model_id,
                        messages=[
                            {"role": "system", "content": "ä½ åªè´Ÿè´£æ— æŸåœ°åœ¨æ–‡æœ¬ä¸­æ’å…¥æ¢è¡Œç¬¦è¿›è¡Œåˆ†é•œï¼Œä¸å‡†è¯´ä»»ä½•åºŸè¯ã€‚"},
                            {"role": "user", "content": director_instruction}
                        ],
                        temperature=0 # é™ä½éšæœºæ€§
                    )
                    st.session_state.processed_text = response.choices[0].message.content
                    st.session_state.desc_results = []
                    st.session_state.current_batch = 0
            except Exception as e:
                st.error(f"å¤„ç†å‡ºé”™: {str(e)}")

# å±•ç¤ºä¸æ ¡éªŒ
if st.session_state.processed_text:
    col_edit, col_dash = st.columns([3, 2])
    
    with col_edit:
        st.subheader("âœï¸ å¯¼æ¼”ç²¾ä¿®åŒº")
        final_edit = st.text_area("åˆ†é•œé¢„è§ˆ (æ¯è¡Œä»£è¡¨ä¸€ä¸ª5ç§’åˆ†é•œ)", value=st.session_state.processed_text, height=450)
        st.session_state.processed_text = final_edit

    with col_dash:
        st.subheader("ğŸ“ˆ å­—æ•°ä¸æ— æŸç›‘æ§")
        lines = [l.strip() for l in final_edit.split('\n') if l.strip()]
        
        # éªŒè¯æ–‡æœ¬æ˜¯å¦å®Œæ•´
        reconstructed = "".join([re.sub(r'^\d+[\.ã€\s]+', '', l) for l in lines])
        orig_len = len(st.session_state.original_stream)
        curr_len = len(reconstructed)
        
        if orig_len == curr_len:
            st.success(f"âœ… æ— æŸæ ¸å¯¹ä¸€è‡´ (å…±{curr_len}å­—)")
        else:
            diff = orig_len - curr_len
            st.error(f"âš ï¸ æ–‡æœ¬ä¸åŒ¹é…ï¼åŸ:{orig_len}å­—, ç°:{curr_len}å­— (å·®é¢:{diff})")

        # åˆ†ææ¯ä¸€è¡Œ
        analysis = []
        for i, l in enumerate(lines):
            c = re.sub(r'^\d+[\.ã€\s]+', '', l)
            analysis.append({"åˆ†é•œ": i+1, "å­—æ•°": len(c), "è¯„ä¼°": "ğŸŸ¢ å®Œç¾" if 25<=len(c)<=40 else "âš ï¸ è°ƒæ•´"})
        st.dataframe(pd.DataFrame(analysis), use_container_width=True)

    st.divider()

    # ================= ç¬¬äºŒé˜¶æ®µï¼šåˆ†æ­¥æè¿°ç”Ÿæˆ =================
    st.header("Step 2: ç”Ÿæˆç”»é¢æè¿°ä¸è§†é¢‘åŠ¨æ€è¯")
    
    char_config = st.text_area("è¾“å…¥æ ¸å¿ƒè§’è‰²è§†è§‰è®¾å®š", placeholder="ä¾‹å¦‚ï¼šæ—å‡¡ï¼š25å²ï¼Œèº«ç©¿é»‘è‰²çš®è¡£ï¼Œçœ¼ç¥å†·å³»...")
    
    if char_config:
        clean_lines = [re.sub(r'^\d+[\.ã€\s]+', '', l.strip()) for l in lines]
        total_len = len(clean_lines)
        batch_idx = st.session_state.current_batch
        size = 20
        end_idx = min(batch_idx + size, total_len)

        if batch_idx < total_len:
            if st.button(f"ğŸ¨ ç”Ÿæˆæ‰¹æ¬¡æè¿° ({batch_idx+1}-{end_idx})"):
                try:
                    client = OpenAI(api_key=api_key, base_url=base_url)
                    batch_text = "\n".join([f"åˆ†é•œ{i+batch_idx+1}: {t}" for i, t in enumerate(clean_lines[batch_idx:end_idx])])
                    
                    desc_prompt = f"""ä½ ç°åœ¨æ˜¯è§†è§‰å¯¼æ¼”ã€‚è¯·ä¸ºä»¥ä¸‹åˆ†é•œç”ŸæˆMJæç¤ºè¯å’Œå³æ¢¦AIæè¿°ã€‚

è§’è‰²è®¾å®šï¼š{char_config}

ä»»åŠ¡ï¼š
1. **ç”»é¢æè¿° (MJ)**ï¼šé™æ€æè¿°ã€‚åœºæ™¯ã€äººç‰©é•¿ç›¸ç»†èŠ‚ã€ç€è£…ã€ç¯å¢ƒå…‰å½±ã€‚ä¸¥ç¦åŠ¨ä½œã€‚
2. **è§†é¢‘ç”Ÿæˆ (å³æ¢¦AI)**ï¼šåŠ¨æ€æè¿°ã€‚æè¿°è¿™5ç§’å†…äººç‰©çš„ç¥æ€ã€å¾®åŠ¨ä½œã€é•œå¤´æ¨ç§»ã€‚
3. **åˆ†é•œé€‚é…**ï¼šç”±äºç›®å‰æ¯ä¸ªåˆ†é•œæ–‡æ¡ˆè¾ƒé•¿ï¼ˆçº¦30-40å­—ï¼‰ï¼Œè¯·åœ¨è§†é¢‘æè¿°ä¸­é€šè¿‡â€œçŸ­å¥å †ç Œâ€å±•ç°å‡ºè¿ç»­çš„åŠ¨ä½œæ„Ÿï¼Œä¸è¦åªåšä¸€ä¸ªåŠ¨ä½œã€‚

åˆ†é•œæ–‡æ¡ˆï¼š
{batch_text}"""
                    
                    response = client.chat.completions.create(
                        model=model_id,
                        messages=[{"role": "user", "content": desc_prompt}]
                    )
                    st.session_state.desc_results.append(response.choices[0].message.content)
                    st.session_state.current_batch = end_idx
                    st.rerun()
                except Exception as e:
                    st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        
        for r in st.session_state.desc_results:
            st.markdown(r)
            st.divider()
