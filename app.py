import streamlit as st
from openai import OpenAI
import re
import time

# ====================
# 1. é¡µé¢é…ç½®ä¸é˜²å´©æºƒåˆå§‹åŒ–
# ====================
st.set_page_config(
    page_title="å…¨èƒ½æ–‡æ¡ˆÂ·ç”µå½±æ„Ÿåˆ†é•œç³»ç»Ÿ (V15 ç»ˆæä¿®æ­£ç‰ˆ)",
    page_icon="ğŸ¬",
    layout="wide"
)

# åˆå§‹åŒ–Session State (é˜²æ­¢åˆ·æ–°æŠ¥é”™)
if 'init' not in st.session_state:
    st.session_state['init'] = True
    st.session_state['result'] = ""
    st.session_state['orig_len'] = 0
    st.session_state['final_len'] = 0
    st.session_state['deviation'] = 0
    st.session_state['shots'] = 0
    st.session_state['chunks'] = 0

st.markdown("""
<style>
    .main-header {font-size: 2rem; font-weight: bold; margin-bottom: 1rem;}
    .stat-box {
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        text-align: center;
        border: 1px solid #ddd;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .stat-value {font-size: 2rem; font-weight: bold; color: #333;}
    .stat-label {font-size: 0.9rem; color: #666;}
    textarea {
        font-family: 'Courier New', Courier, monospace; 
        font-size: 16px !important;
        line-height: 1.8 !important;
    }
    .stProgress > div > div > div > div {
        background-color: #28a745;
    }
</style>
""", unsafe_allow_html=True)

# ====================
# 2. ä¾§è¾¹æ 
# ====================
with st.sidebar:
    st.markdown("### âš™ï¸ å¯¼æ¼”å¼•æ“ V15 (ä¸¥è°¨ç‰ˆ)")
    api_key = st.text_input("API Key", type="password")
    base_url = st.text_input("æ¥å£åœ°å€", value="https://blog.tuiwen.xyz/v1")
    
    model_options = ["deepseek-chat", "gpt-4o", "claude-3-5-sonnet", "gemini-pro", "grok-beta", "è‡ªå®šä¹‰"]
    selected_model = st.selectbox("Model ID", model_options)
    
    if selected_model == "è‡ªå®šä¹‰":
        model_id = st.text_input("è¾“å…¥æ¨¡å‹åç§°", value="grok-4.1")
    else:
        model_id = selected_model
        
    st.info("â„¹ï¸ V15æ ¸å¿ƒå‡çº§ï¼š\n1. æ™ºèƒ½æ•´å¥åˆ‡åˆ†(é˜²æ­¢æ–­å¥ä¸¢å­—)\n2. é€å­—å¤åˆ»æ¨¡å¼(ç¦æ­¢AIæ€»ç»“)")

# ====================
# 3. æ ¸å¿ƒé€»è¾‘ï¼šæ™ºèƒ½åˆ‡åˆ†ä¸å¤„ç†
# ====================

def clean_text_for_count(text):
    """çº¯å‡€å­—æ•°ç»Ÿè®¡ï¼ˆå»æ ‡ç‚¹ï¼‰"""
    if not text: return ""
    pattern = re.compile(r'[^\u4e00-\u9fa5a-zA-Z0-9]')
    return re.sub(pattern, '', text)

def smart_sentence_split(text, max_chars=600):
    """
    ã€V15 æ™ºèƒ½æ•´å¥åˆ‡åˆ†ç®—æ³•ã€‘
    ç»å¯¹ä¸åˆ‡æ–­å¥å­ã€‚å¯»æ‰¾å¥å·ã€æ„Ÿå¹å·ã€é—®å·è¿›è¡Œå®‰å…¨åˆ†å‰²ã€‚
    ä¿è¯æ¯ä¸€å—å‘ç»™AIçš„éƒ½æ˜¯å®Œæ•´çš„è¯­ä¹‰å—ã€‚
    """
    chunks = []
    current_chunk = ""
    
    # 1. å…ˆç”¨æ­£åˆ™æŒ‰å¥å­ç»“æŸç¬¦åˆ‡åˆ† (ä¿ç•™åˆ†éš”ç¬¦)
    # åŒ¹é… ã€‚ï¼ï¼Ÿ ä»¥åŠæ¢è¡Œç¬¦ï¼Œæ”¾åˆ°åˆ—è¡¨ä¸­
    parts = re.split(r'(ã€‚|ï¼|ï¼Ÿ|\n)', text)
    
    temp_sentence = ""
    
    # 2. é‡æ–°ç»„è£…
    for part in parts:
        temp_sentence += part
        # å¦‚æœ part æ˜¯æ ‡ç‚¹ï¼Œè¯´æ˜ä¸€ä¸ªå¥å­ç»“æŸäº†
        if part in ["ã€‚", "ï¼", "ï¼Ÿ", "\n"]:
            # æ£€æŸ¥åŠ å…¥å½“å‰å—æ˜¯å¦ä¼šè¶…æ ‡
            if len(current_chunk) + len(temp_sentence) > max_chars:
                # è¶…æ ‡äº†ï¼Œå…ˆä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk)
                # æ–°èµ·ä¸€å—
                current_chunk = temp_sentence
            else:
                # æ²¡è¶…æ ‡ï¼Œæ¥ä¸Šå»
                current_chunk += temp_sentence
            temp_sentence = ""
            
    # å¤„ç†å‰©ä½™éƒ¨åˆ†
    if temp_sentence:
        current_chunk += temp_sentence
    if current_chunk:
        chunks.append(current_chunk)
        
    return chunks

def process_chunk_strict(client, model, text_chunk, index, total):
    """
    ã€V15 é€å­—å¤åˆ» Promptã€‘
    æ ¸å¿ƒæ”¹å˜ï¼šä¸å†è¦æ±‚AIâ€œç†è§£å‰§æƒ…â€ï¼Œè€Œæ˜¯è¦æ±‚å®ƒåšâ€œæ’ç‰ˆå·¥â€ã€‚
    è¿™èƒ½æœ€å¤§ç¨‹åº¦é˜²æ­¢AIæ”¹å†™æˆ–é—æ¼å†…å®¹ã€‚
    """
    system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ã€å­—å¹•æ’ç‰ˆå¼•æ“ã€‘ã€‚ä½ çš„ä»»åŠ¡ä¸æ˜¯å†™ä½œï¼Œè€Œæ˜¯å¯¹ç°æœ‰æ–‡æœ¬è¿›è¡Œæ¢è¡Œæ’ç‰ˆã€‚

ã€ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹ 3 æ¡æ­»å‘½ä»¤ã€‘ï¼š
1. **é€å­—å¤åˆ»**ï¼šè¾“å‡ºçš„å†…å®¹å¿…é¡»ä¸è¾“å…¥å®Œå…¨ä¸€è‡´ï¼Œ**ç¦æ­¢ä¿®æ”¹ã€åˆ é™¤ã€å¢åŠ ä»»ä½•ä¸€ä¸ªæ±‰å­—**ã€‚ç¦æ­¢è¿›è¡Œæ‘˜è¦æˆ–æ€»ç»“ï¼
2. **æ’ç‰ˆé€»è¾‘**ï¼š
   - å°†é•¿æ®µè½æ‹†è§£ä¸ºçŸ­å¥ï¼Œæ¯è¡Œ **25-35ä¸ªå­—ç¬¦**ã€‚
   - é‡åˆ°å¯¹è¯ã€åŠ¨ä½œåˆ‡æ¢æ—¶æ¢è¡Œã€‚
   - å¦‚æœä¸€å¥è¯å¤ªçŸ­ï¼ˆå°‘äº10å­—ï¼‰ä¸”ä¸ä¸‹ä¸€å¥å…³è”ç´§å¯†ï¼Œè¯·åˆå¹¶åˆ°åŒä¸€è¡Œï¼Œä½†ä¸è¦è¶…è¿‡35å­—ã€‚
3. **è¾“å‡ºæ ¼å¼**ï¼šçº¯æ–‡æœ¬ï¼Œæ¯è¡Œå¼€å¤´æ ‡æ•°å­—åºå·ã€‚

è¾“å…¥æ–‡æœ¬ç‰‡æ®µ ({index+1}/{total})ï¼š
{text_chunk}

è¯·è¾“å‡ºæ’ç‰ˆåçš„ç»“æœï¼š
"""
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": text_chunk}
            ],
            stream=False,
            temperature=0.1 # æä½æ¸©åº¦ï¼Œæ‰¼æ€AIçš„åˆ›é€ æ¬²ï¼Œåªä¿ç•™æ‰§è¡ŒåŠ›
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Error: {str(e)}"

# ====================
# 4. ä¸»ç¨‹åº
# ====================

st.markdown('<div class="main-header">ğŸ¬ å…¨èƒ½æ–‡æ¡ˆÂ·ç”µå½±æ„Ÿåˆ†é•œç³»ç»Ÿ (V15)</div>', unsafe_allow_html=True)

uploaded_file = st.file_uploader("ğŸ“‚ é€‰æ‹© TXT æ–‡æ¡ˆ", type=['txt'])

if uploaded_file and api_key:
    raw_content = uploaded_file.read().decode("utf-8")
    # å»é™¤å¤šä½™ç©ºæ ¼å’Œç©ºè¡Œï¼Œæ•´ç†æˆç´§å‡‘æµ
    clean_input = re.sub(r'\s+', '', raw_content).replace("\n", "")
    # æ¢å¤æ ‡ç‚¹åçš„è‡ªç„¶åœé¡¿ï¼ˆå¯é€‰ï¼Œé˜²æ­¢å¤ªå¯†ï¼‰
    # ä½†ä¸ºäº†ç»å¯¹åŒ¹é…ï¼Œæˆ‘ä»¬ç›´æ¥å¤„ç† clean_input
    
    orig_len = len(clean_text_for_count(clean_input))
    st.caption(f"ğŸ“„ åŸæ–‡å·²åŠ è½½ï¼Œå…± {orig_len} çº¯å‡€å­—ç¬¦ã€‚")

    if st.button("ğŸš€ å¯åŠ¨é«˜ä¿çœŸåˆ†é•œ", type="primary"):
        
        # 1. æ™ºèƒ½åˆ‡åˆ†
        chunks = smart_sentence_split(raw_content, max_chars=500) # 500å­—æ›´å®‰å…¨
        total_chunks = len(chunks)
        
        # éªŒè¯åˆ‡åˆ†æ˜¯å¦ä¸¢å­—
        test_len = sum([len(c) for c in chunks])
        # st.write(f"åˆ‡åˆ†å®Œæ•´æ€§æ£€æŸ¥: åŸæ–‡{len(raw_content)} vs åˆ‡å—æ€»å’Œ{test_len}") 
        
        progress_bar = st.progress(0)
        status_box = st.empty()
        full_results = []
        
        client = OpenAI(api_key=api_key, base_url=base_url)
        
        start_time = time.time()
        
        # 2. é€å—å¤„ç†
        for i, chunk in enumerate(chunks):
            status_box.markdown(f"**âš¡ æ­£åœ¨æ’ç‰ˆç¬¬ {i+1}/{total_chunks} åŒºå—...** (ä¸¥è°¨æ¨¡å¼)")
            
            res = process_chunk_strict(client, model_id, chunk, i, total_chunks)
            
            if "Error" in res:
                st.error(f"å¤„ç†ä¸­æ–­: {res}")
                break
                
            # æ¸…æ´—AIè¾“å‡ºçš„åºå·
            lines = res.split('\n')
            for line in lines:
                # æå–å†…å®¹ï¼Œå»é™¤é¦–å°¾ç©ºç™½
                # å…¼å®¹æ ¼å¼: "1. å†…å®¹" æˆ– "1ã€å†…å®¹" æˆ– "1 å†…å®¹"
                cleaned = re.sub(r'^[\d\s\.\ã€]+', '', line).strip()
                if cleaned:
                    full_results.append(cleaned)
            
            progress_bar.progress((i + 1) / total_chunks)
            
        # 3. ç»“æœç»„è£…
        if full_results:
            final_text = ""
            combined_clean_text = ""
            for idx, txt in enumerate(full_results):
                final_text += f"{idx + 1}.{txt}\n"
                combined_clean_text += txt
            
            # å­˜å…¥çŠ¶æ€
            st.session_state['result'] = final_text
            st.session_state['orig_len'] = orig_len
            st.session_state['final_len'] = len(clean_text_for_count(combined_clean_text))
            st.session_state['deviation'] = st.session_state['final_len'] - orig_len
            st.session_state['shots'] = len(full_results)
            st.session_state['chunks'] = total_chunks
            
            status_box.success("âœ… å…¨é‡æ’ç‰ˆå®Œæˆï¼")
            time.sleep(0.5)
            st.rerun()

# ====================
# 5. ç»“æœé¢æ¿
# ====================

if st.session_state['result']:
    result = st.session_state['result']
    orig = st.session_state['orig_len']
    final = st.session_state['final_len']
    dev = st.session_state['deviation']
    shots = st.session_state['shots']
    
    st.markdown("---")
    
    # ç»Ÿè®¡å¡ç‰‡
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.markdown(f'<div class="stat-box"><div class="stat-label">åŸæ–‡çº¯å­—æ•°</div><div class="stat-value">{orig}</div></div>', unsafe_allow_html=True)
    with c2:
        st.markdown(f'<div class="stat-box"><div class="stat-label">ç”Ÿæˆåˆ†é•œæ€»æ•°</div><div class="stat-value">{shots} ç»„</div></div>', unsafe_allow_html=True)
    with c3:
        st.markdown(f'<div class="stat-box"><div class="stat-label">å¤„ç†åçº¯å­—æ•°</div><div class="stat-value">{final}</div></div>', unsafe_allow_html=True)
    with c4:
        # åå·®é€»è¾‘ï¼šå…è®¸æå°è¯¯å·®ï¼ˆå¯èƒ½æ˜¯AIæŠŠè‹±æ–‡æ ‡ç‚¹è½¬ä¸­æ–‡æ ‡ç‚¹å¯¼è‡´çš„æ­£åˆ™è¯¯å·®ï¼‰
        if abs(dev) < 10:
            color = "#28a745"
            msg = "å®Œç¾"
        elif abs(dev) < 50:
            color = "#ffc107"
            msg = "è½»å¾®åå·®"
        else:
            color = "#dc3545"
            msg = "ä¸¥é‡åå·®"
        st.markdown(f'<div class="stat-box"><div class="stat-label">åå·®å€¼ ({msg})</div><div class="stat-value" style="color:{color}">{dev} å­—</div></div>', unsafe_allow_html=True)
        
    if dev < -50:
        st.error(f"âš ï¸ ä¾ç„¶å­˜åœ¨ä¸¢å­—ç°è±¡ (-{abs(dev)})ï¼Ÿè¿™é€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹ä¸å¤Ÿæ™ºèƒ½ã€‚å»ºè®®ä½¿ç”¨ GPT-4o æˆ– Claude-3.5ï¼Œå®ƒä»¬å¯¹â€œé€å­—å¤åˆ»â€æŒ‡ä»¤çš„æ‰§è¡ŒåŠ›è¿œå¼ºäº grok/deepseek-chatã€‚")

    st.markdown("### ğŸ“ è§†è§‰åˆ†é•œç¼–è¾‘å™¨")
    st.text_area("åˆ†é•œç»“æœ", value=result, height=800)
    st.download_button("ğŸ“¥ ä¸‹è½½ .txt", result, "final_storyboard.txt")
